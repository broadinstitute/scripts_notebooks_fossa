{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pycytominer\n",
    "import easygui as eg\n",
    "import sys\n",
    "import hdbscan\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\Fer\\Documents\\GitHub\")\n",
    "from scripts_notebooks_fossa.pycombat_umap import combat_util\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = eg.fileopenbox(msg=\"Choose a file\", default=r\"F:\")\n",
    "print('Filename', myfile)\n",
    "df = pd.read_csv(myfile)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_input = 15\n",
    "min_dist_input = 0.5\n",
    "metric = 'cosine'\n",
    "hover_list = ['Metadata_Plate','Metadata_Well', 'Metadata_compound', 'Metadata_concentration_uM']\n",
    "number_of_iterations=50\n",
    "true_labels = \"Metadata_compound\"\n",
    "size_col=\"Metadata_NPSize_nm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_join = [\"Metadata_compound\", \"Metadata_concentration_uM\"]\n",
    "df, new_col = combat_util.col_generator(df, cols_to_join = cols_to_join)\n",
    "\n",
    "# #just remove the 0 for the non-treated wells\n",
    "# df[new_col] = df[new_col].str.replace(r' 0', ' 20', regex=True)\n",
    "# df[new_col].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out treatments\n",
    "\n",
    "If you've performed any profile evaluation and knows which profiles are not technically reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_out=['Orphenadrine 1', 'Non-treated 0', 'Lactose 1', 'Lactose 10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.query(f'{new_col} not in {filter_out}').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare X and Y and UMAP vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pycytominer.cyto_utils.features.infer_cp_features(df_filtered, metadata=True)\n",
    "feat = [x for x in df_filtered.columns.tolist() if x not in meta]\n",
    "X = pd.DataFrame(df_filtered, columns=feat)\n",
    "y = pd.DataFrame(df_filtered, columns=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3d = combat_util.generate_x_y_umap(df_filtered, n_neighbors=n_neighbors_input, \n",
    "                                                  min_dist=min_dist_input, metric=metric, iterate=True, \n",
    "                                                  number_runs=number_of_iterations, n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_plot = combat_util.plot_umap_3d(df_3d, color_col='Metadata_compound', \n",
    "                    #   split_df = False, split_column = None, np = None,\n",
    "                      hover_cols=hover_list,\n",
    "                      size=True, size_col=size_col,\n",
    "                      # x=\"0\", y=\"1\",\n",
    "                      # error_x=\"x_err\", error_y=\"y_err\",\n",
    "                       dili_color=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PCA clustering, then HDSBCAN\n",
    "\n",
    "For the sake of performance we’ll reduce the dimensionality of the data down to 50 dimensions via PCA (this recovers most of the variance), since HDBSCAN scales somewhat poorly with the dimensionality of the data it will work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "lowd_df = pca.fit_transform(X)\n",
    "hdbscan_labels = hdbscan.HDBSCAN(min_cluster_size=7).fit_predict(lowd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The resulting array contains the cumulative percentage of variance explained by the first i principal components, where i ranges from 1 to k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentage of variance explained by each component\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative sum of the explained variance ratios\n",
    "cumulative_variance_ratio = np.cumsum(variance_ratio)\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.plot(cumulative_variance_ratio)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  it should be noted that one of the features of HDBSCAN is that it can refuse to cluster some points and classify them as “noise”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3d[\"Metadata_hdbscan_label\"] = hdbscan_labels\n",
    "\n",
    "classical_plot\n",
    "combat_util.plot_umap_3d(df_3d, color_col='Metadata_hdbscan_label', \n",
    "                    #   split_df = False, split_column = None, np = None,\n",
    "                      hover_cols=hover_list,\n",
    "                      size=True, size_col = size_col,\n",
    "                      # x=\"0\", y=\"1\",\n",
    "                      # error_x=\"x_err\", error_y=\"y_err\",\n",
    "                       dili_color=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 representing a bad (essentially random) clustering and 1 representing perfectly recovering the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    adjusted_rand_score(y[true_labels], hdbscan_labels),\n",
    "    adjusted_mutual_info_score(y[true_labels], hdbscan_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can instead only look at the subset of the data that HDBSCAN was actually confident enough to assign to clusters – a simple sub-selection will let us recompute the scores for only that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = (hdbscan_labels >= 0)\n",
    "(\n",
    "    adjusted_rand_score(y[true_labels][clustered], hdbscan_labels[clustered]),\n",
    "    adjusted_mutual_info_score(y[true_labels][clustered], hdbscan_labels[clustered])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much of the data did HDBSCAN actually assign to clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(clustered) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Inside HDBSCAN hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_samples=6, min_cluster_size=10, metric='euclidean').fit(result_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the hierarchy as a dendrogram, the width (and color) of each branch representing the number of points in the cluster at that level. If we wish to know which branches were selected by the HDBSCAN* algorithm we can pass select_clusters=True. You can even pass a selection palette to color the selections according to the cluster labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.condensed_tree_.plot(select_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.single_linkage_tree_.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. UMAP clustering, then HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the UMAP vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the three columns\n",
    "selected_columns = df_3d[['0', '1', '2']]\n",
    "\n",
    "# Convert to a NumPy array\n",
    "result_array = selected_columns.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Run with cosine metric\n",
    "\n",
    "Uncomment the lines below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import distance\n",
    "\n",
    "# # define X (n_samples, n_features)\n",
    "# mat = distance.cdist(result_array, result_array, metric='cosine')\n",
    "# hdb = hdbscan.HDBSCAN(min_samples=6,min_cluster_size=10,metric='precomputed')\n",
    "# hdbscan_labels=hdb.fit_predict(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 OR run with euclidean metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_labels = hdbscan.HDBSCAN(min_samples=6, min_cluster_size=10, metric='euclidean').fit_predict(result_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3d[\"Metadata_hdbscan_label\"] = hdbscan_labels\n",
    "\n",
    "\n",
    "classical_plot\n",
    "combat_util.plot_umap_3d(df_3d, color_col='Metadata_hdbscan_label', \n",
    "                    #   split_df = False, split_column = None, np = None,\n",
    "                      hover_cols=hover_list,\n",
    "                      size=True, size_col = size_col,\n",
    "                      # x=\"0\", y=\"1\",\n",
    "                      # error_x=\"x_err\", error_y=\"y_err\",\n",
    "                       discrete=True\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 representing a bad (essentially random) clustering and 1 representing perfectly recovering the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    adjusted_rand_score(y[true_labels], hdbscan_labels),\n",
    "    adjusted_mutual_info_score(y[true_labels], hdbscan_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can instead only look at the subset of the data that HDBSCAN was actually confident enough to assign to clusters – a simple sub-selection will let us recompute the scores for only that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = (hdbscan_labels >= 0)\n",
    "(\n",
    "    adjusted_rand_score(y[true_labels][clustered], hdbscan_labels[clustered]),\n",
    "    adjusted_mutual_info_score(y[true_labels][clustered], hdbscan_labels[clustered])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much of the data did HDBSCAN actually assign to clusters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(clustered) / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Inside HDBSCAN hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_samples=6, min_cluster_size=10, metric='euclidean').fit(result_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the hierarchy as a dendrogram, the width (and color) of each branch representing the number of points in the cluster at that level. If we wish to know which branches were selected by the HDBSCAN* algorithm we can pass select_clusters=True. You can even pass a selection palette to color the selections according to the cluster labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.condensed_tree_.plot(select_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.single_linkage_tree_.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. kmeans clustering for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=20, algorithm=\"elkan\")\n",
    "kmeans_labels = kmeans.fit_predict(result_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3d[\"Metadata_kmeans_label\"] = kmeans_labels\n",
    "\n",
    "classical_plot\n",
    "combat_util.plot_umap_3d(df_3d, color_col='Metadata_kmeans_label', \n",
    "                    #   split_df = False, split_column = None, np = None,\n",
    "                      hover_cols=hover_list,\n",
    "                      size=True, size_col=size_col,\n",
    "                      # x=\"0\", y=\"1\",\n",
    "                      # error_x=\"x_err\", error_y=\"y_err\",\n",
    "                       discrete=True\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "\n",
    "ARI = 1: Perfect clustering.\n",
    "\n",
    "ARI = 0: Clustering is no better than random.\n",
    "\n",
    "ARI = -1: Perfect disagreement between true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Assuming y_true is the true cluster assignments and kmeans_labels is the predicted labels from K-means\n",
    "ari = adjusted_rand_score(y[true_labels], kmeans_labels)\n",
    "nmi = normalized_mutual_info_score(y[true_labels], kmeans_labels)\n",
    "\n",
    "print(\"Adjusted Rand Index:\", ari)\n",
    "print(\"Normalized Mutual Information:\", nmi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycytominer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
